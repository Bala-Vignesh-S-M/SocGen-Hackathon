{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Societe Generale Hackathon\n"
      ],
      "metadata": {
        "id": "olzhaDIHe0t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greetings, we've opted for the third problem statement: \"**Matching of job openings with skill database using machine learning**\"\n",
        "\n",
        "Our approach involves employing the **word2vec** model on a dataset sourced from data science websites. The primary objective is to extract skillsets, specifically various collections of data science skills. Following this skillset extraction, we proceeded to process resumes (in PDF format) using **Optical Character Recognition (OCR)** to extract textual content.\n",
        "\n",
        "This extracted content is then compared against job prerequisites, specifically the required skills, to determine the suitability of the individual for the given role. To conduct this assessment, we take both the job requirements and the resume as inputs, evaluating their alignment.\n",
        "\n",
        "It's important to note that our current dataset only covers data science skills due to the data scraped from data science websites. However, this methodology has the potential to be extended to encompass roles beyond the scope of data science.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L7O6n-thfqxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required modules\n"
      ],
      "metadata": {
        "id": "f5vtDcrjfGm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pdf2image\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install python-poppler\n",
        "!pip install PyMuPDF\n",
        "!pip install Aspose.Email-for-Python-via-NET\n",
        "!pip install aspose-words"
      ],
      "metadata": {
        "id": "Xu-aS-oBd6pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217cc499-4e9c-49b8-c4a2-ee3646e4610c"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: pdfminer.six==20221105 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20221105)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.18.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 109 kB in 2s (44.4 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-poppler\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.1)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.0 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.0)\n",
            "Requirement already satisfied: Aspose.Email-for-Python-via-NET in /usr/local/lib/python3.10/dist-packages (23.7)\n",
            "Requirement already satisfied: aspose-words in /usr/local/lib/python3.10/dist-packages (23.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries"
      ],
      "metadata": {
        "id": "Znb3HRTIfVwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "y3BMAOXaLQEq"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount drive"
      ],
      "metadata": {
        "id": "OMaMIfMkfdxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRibkqUSL-Fo",
        "outputId": "5c1d237e-fb95-4d6e-a539-6710b4f80473"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the word2vec model"
      ],
      "metadata": {
        "id": "u8plug-eeEUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/data_100.csv')\n",
        "sent = [row.split() for row in df['data']]\n",
        "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "sentences=phrases[sent]"
      ],
      "metadata": {
        "id": "5-1xhaGFMTSj"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "window=3,\n",
        "vector_size=300,\n",
        "sample=6e-5,\n",
        "alpha=0.03,\n",
        "min_alpha=0.0007,\n",
        "negative=20\n",
        ")\n",
        "\n",
        "#Building Vocabulary\n",
        "w2v_model.build_vocab(sentences)\n",
        "\n",
        "#Saving the built vocabulary locally\n",
        "vocab_keys = list(w2v_model.wv.key_to_index.keys())\n",
        "vocab_df = pd.DataFrame({'word': vocab_keys})\n",
        "vocab_df.to_csv('vocabulary.csv', index=False)\n",
        "\n",
        "#Training the model\n",
        "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 30, report_delay = 1)\n",
        "\n",
        "#saving the model\n",
        "path = \"/content/drive/MyDrive/SocGen/w2v_model.pkl\"\n",
        "joblib.dump(w2v_model, path)\n",
        "\n",
        "print(w2v_model.wv.similarity('neural_network', 'machine_learning'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXJftamMAZ6",
        "outputId": "43a15135-3771-4dec-cff9-e93b59e0e924"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6653827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'phrases_split.pkl'\n",
        "drive_path = '/content/drive/MyDrive/SocGen/phrases_split.pkl'\n",
        "\n",
        "# Save the object using joblib\n",
        "joblib.dump(phrases, path)\n",
        "joblib.dump(phrases, drive_path)"
      ],
      "metadata": {
        "id": "QdKl-FYRWJkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac80c3e-9437-47c3-afc0-28b789653f04"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/SocGen/phrases_split.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skill Matching using word2vec"
      ],
      "metadata": {
        "id": "9C8BCDZHeIv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demo.py\n",
        "import sys\n",
        "import gensim\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "import numpy as np\n",
        "from itertools import groupby, count\n",
        "import re\n",
        "import subprocess\n",
        "import os.path\n",
        "import sys\n",
        "import logging\n",
        "import joblib\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import pytesseract\n",
        "import cv2\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = 1000000000\n",
        "import aspose.words as aw\n",
        "import fitz\n",
        "def _skills_in_box(image_gray,threshold=60):\n",
        "  '''\n",
        "  Function for identifying boxes and identifying skills in it: Given an imge path,\n",
        "        returns string with text in it.\n",
        "        Parameters:\n",
        "            img_path: Path of the image\n",
        "            thresh : Threshold of the box to convert it to 0\n",
        "  '''\n",
        "  img = image_gray.copy()\n",
        "  thresh_inv = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
        "  # Blur the image\n",
        "  blur = cv2.GaussianBlur(thresh_inv,(1,1),0)\n",
        "  thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
        "  # find contours\n",
        "  contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
        "  mask = np.ones(img.shape[:2], dtype=\"uint8\") * 255\n",
        "  available = 0\n",
        "  for c in contours:\n",
        "    # get the bounding rect\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    if w*h>1000:\n",
        "        cv2.rectangle(mask, (x+5, y+5), (x+w-5, y+h-5), (0, 0, 255), -1)\n",
        "        available = 1\n",
        "\n",
        "  res = ''\n",
        "  if available == 1:\n",
        "    res_final = cv2.bitwise_and(img, img, mask=cv2.bitwise_not(mask))\n",
        "    res_final[res_final<=threshold]=0\n",
        "    kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
        "    res_fin = cv2.filter2D(src=res_final, ddepth=-1, kernel=kernel)\n",
        "    vt = pytesseract.image_to_data(255-res_final,output_type='data.frame')\n",
        "    vt = vt[vt.conf != -1]\n",
        "    res = ''\n",
        "    for i in vt[vt['conf']>=43]['text']:\n",
        "      res = res + str(i) + ' '\n",
        "  print(res)\n",
        "  return res\n",
        "\n",
        "def _image_to_string(img):\n",
        "  '''\n",
        "  Function for converting images to grayscale and converting to text: Given an image path,\n",
        "  returns text in it.\n",
        "  Parameters:\n",
        "      img_path: Path of the image\n",
        "  '''\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  res = ''\n",
        "  string1 = pytesseract.image_to_data(img,output_type='data.frame')\n",
        "  string1 = string1[string1['conf'] != -1]\n",
        "  for i in string1[string1['conf']>=43]['text']:\n",
        "    res = res + str(i) + ' '\n",
        "  string3 = _skills_in_box(img)\n",
        "  return res+string3\n",
        "\n",
        "def _pdf_to_png(pdf_path):\n",
        "    '''\n",
        "    Function for converting pdf to image and saves it in a folder and\n",
        "    convert the image into string\n",
        "    Parameter:\n",
        "        pdf_path: Path of the pdf\n",
        "    '''\n",
        "    string = ''\n",
        "    images = convert_from_path(pdf_path)\n",
        "    for j in tqdm(range(len(images))):\n",
        "        # Save pages as images in the pdf\n",
        "        image = np.array(images[j])\n",
        "        string += _image_to_string(image)\n",
        "        string += '\\n'\n",
        "    return string\n",
        "def ocr(paths):\n",
        "    '''\n",
        "    Function for checking the pdf is image or not. If the file is in .doc it converts it into .pdf\n",
        "    if the pdf is in image format the function converts .pdf to .png\n",
        "    Parameter:\n",
        "        paths: list containg paths of all pdf files\n",
        "    '''\n",
        "    text = \"\"\n",
        "    res = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(paths)\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        if len(text) <=10 :\n",
        "            res = _pdf_to_png(paths)\n",
        "        else:\n",
        "            res = text\n",
        "    except:\n",
        "        doc = aw.Document(paths)\n",
        "        doc.save(\"Document.pdf\")\n",
        "        doc = fitz.open(\"Document.pdf\")\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        if len(text) <=10 :\n",
        "            res = _pdf_to_png(\"Document.pdf\")\n",
        "        else:\n",
        "            res = text\n",
        "        os.remove(\"Document.pdf\")\n",
        "    return res\n",
        "\n",
        "def to_la(L):\n",
        "  k=list(L)\n",
        "  l=np.array(k)\n",
        "  return l.reshape(-1, 1)\n",
        "\n",
        "def cos(A, B):\n",
        "  dot_prod=np.matmul(A,B.T)\n",
        "  norm_a=np.reciprocal(np.sum(np.abs(A)**2,axis=-1)**(1./2))\n",
        "  norm_b=np.reciprocal(np.sum(np.abs(B)**2,axis=-1)**(1./2))\n",
        "  norm_a=to_la(norm_a)\n",
        "  norm_b=to_la(norm_b)\n",
        "  k=np.matmul(norm_a,norm_b.T)\n",
        "  return list(np.multiply(dot_prod,k))\n",
        "\n",
        "def check(path,skills,l2,w2v_model1,phrases,pattern):\n",
        "  text = ocr(path)\n",
        "  text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
        "  text = text.lower()\n",
        "  text = re.sub(\"\\\\\\|,|/|:|\\)|\\(\",\" \",text)\n",
        "  t2 = text.split()\n",
        "  l_2=l2.copy()\n",
        "  match=list(set(re.findall(pattern,text)))\n",
        "  sentences=phrases[t2]\n",
        "  resume_skills_dict={}\n",
        "  res_jdskill_intersect=list(set(sentences).intersection(set(l_2)))\n",
        "  if(len(match)!=0):\n",
        "    for k in match:\n",
        "      k=k.replace(' ','_')\n",
        "      resume_skills_dict[k]=1\n",
        "      try:\n",
        "        l_2.remove(k)\n",
        "      except:\n",
        "        continue\n",
        "  l6=list(set(l_2).intersection(skills['word']))\n",
        "  l6_minus_skills=list(set(l_2).difference(skills['word']))\n",
        "  for i in l6_minus_skills:\n",
        "    resume_skills_dict[i]=0\n",
        "  if(len(l6)==0):\n",
        "    return resume_skills_dict\n",
        "  l4=list(set(sentences).intersection(skills['word']))\n",
        "  arr1 = np.array([w2v_model1.wv.get_vector(i) for i in l6])\n",
        "  arr2 = np.array([w2v_model1.wv.get_vector(i) for i in l4])\n",
        "  similarity_values=cos(arr1,arr2)\n",
        "  count=0\n",
        "  for i in similarity_values:\n",
        "    k=list(filter(lambda x: x<0.38, list(i)))\n",
        "    if(len(k)==len(i)):\n",
        "      resume_skills_dict[l6[count]]=0\n",
        "    else:\n",
        "      resume_skills=[s for s in range(len(i)) if(i[s])>0.38]\n",
        "      resume_skills_dict[l6[count]]=1\n",
        "    count+=1\n",
        "  return resume_skills_dict\n",
        "\n",
        "def Convert(string):\n",
        "    li = list(string.split())\n",
        "    return list(set(li))\n",
        "\n",
        "def preprocess(string):\n",
        "  string = string.replace(\",\",' ')\n",
        "  string= string.replace(\"'\",' ')\n",
        "  string = Convert(string)\n",
        "  return string\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   #Arg 1 = vocabulary, Arg 2 = model, Arg 3 = phrases object, Arg 4 = JD's Mandatory Skills, Arg 5 = Resume Path\n",
        "   argv = sys.argv[1:]\n",
        "   w2v_model1 = joblib.load(argv[0])\n",
        "   skills=pd.read_csv(argv[1])\n",
        "   mapper = {}\n",
        "   underscore=[]\n",
        "   jd_skills=argv[3]\n",
        "   jd_skills=\" \".join(jd_skills.strip().split())\n",
        "   jd_skills=jd_skills.replace(', ',',')\n",
        "   pattern=jd_skills.replace(',','|').lower()\n",
        "   for i in jd_skills.split(','):\n",
        "    if '_' in i:\n",
        "      underscore.append(i)\n",
        "      mapper[i.lower().replace('_',' ')] = i\n",
        "   jd_skills=jd_skills.replace(' ','_')\n",
        "   jd_skills=jd_skills.replace(',',', ')\n",
        "   for i in jd_skills.split(', '):\n",
        "    if i not in underscore:\n",
        "      if '_' in i:\n",
        "        mapper[i.lower().replace('_',' ')] = i.replace('_',' ')\n",
        "      elif '-' in i:\n",
        "        mapper[i.lower().replace('-',' ')] = i\n",
        "      else:\n",
        "        mapper[i.lower()] = i\n",
        "   jd_skills=jd_skills.replace('-','_')\n",
        "   phrases=Phrases.load(argv[2])\n",
        "   lines = [preprocess(jd_skills.lower().rstrip())]\n",
        "   phrases=Phrases.load(argv[2])\n",
        "   final_jd_skills=list(set(lines[0]).intersection(skills['word']))\n",
        "   path = argv[4]\n",
        "   res=check(path,skills,lines[0],w2v_model1,phrases,pattern)\n",
        "   print('skills_matched :',res)\n",
        "   perc = ((sum(res.values()))*100)//(len(res))\n",
        "   print(f\"Percentage of match: {perc}%\")\n",
        "   if perc > 60:\n",
        "    print(f\"This resume is eligible for the role as it matches {perc} which is more than 60 percent of the job requirements\")\n",
        "   else:\n",
        "    print(f\"This resume is not eligible for the role as it matches {perc} which is less than 60 percent of the job requirements\")"
      ],
      "metadata": {
        "id": "ytr_KBkLRLiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf912c5-89a1-44ff-a0e9-8e51037ca7ac"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting demo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 demo.py '/content/drive/MyDrive/SocGen/w2v_model.pkl' '/content/vocabulary.csv' '/content/drive/MyDrive/SocGen/phrases_split.pkl' 'CNN, OOPs, ml, RDBMS, oracle, python, OpenCV, r, tensorflow, snowflake, sql' '/content/drive/MyDrive/SocGen/Bala-Vignesh-S-M-Updated.pdf'"
      ],
      "metadata": {
        "id": "wmjlIN_aRLgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51415c3e-0aeb-4285-9d0f-28d5411879de"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skills_matched : {'python': 1, 'oops': 1, 'cnn': 1, 'tensorflow': 1, 'opencv': 1, 'r': 1, 'sql': 1, 'rdbms': 0, 'oracle': 0, 'snowflake': 1, 'ml': 1}\n",
            "Percentage of match: 81%\n",
            "This resume is eligible for the role as it matches 81 which is more than 60 percent of the job requirements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you"
      ],
      "metadata": {
        "id": "MKNWXmaDmlwU"
      }
    }
  ]
}